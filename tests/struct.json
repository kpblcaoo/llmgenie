{
  "metadata": {
    "project_name": "llmstruct",
    "description": "Utility for generating structured JSON for codebases",
    "version": "2025-06-12T10:31:05.890001Z",
    "authors": [
      {
        "name": "Mikhail Stepanov",
        "github": "kpblcaoo",
        "email": "kpblcaoo@gmail.com"
      }
    ],
    "instructions": [
      "Follow best practices, warn if instructions conflict with them",
      "Preserve functionality, ensure idempotency",
      "Use attached struct.json for context and navigation",
      "Request missing modules or functions if needed",
      "Regenerate JSON for significant changes, track via Git and artifacts",
      "Use internal comments for descriptions, append brief summary"
    ],
    "goals": [],
    "stats": {
      "modules_count": 15,
      "functions_count": 97,
      "classes_count": 14,
      "call_edges_count": 101
    },
    "artifact_id": "fa246623-b332-4a07-a2ca-f8ab501d4ac8",
    "summary": "Structured JSON for llmstruct codebase",
    "tags": [
      "codebase",
      "automation"
    ],
    "folder_structure": [
      {
        "path": ".",
        "type": "directory",
        "artifact_id": "8c9327e2-9e4a-4990-b65b-6ae3633e4f01",
        "metadata": {}
      },
      {
        "path": ".llmstruct_index",
        "type": "directory",
        "artifact_id": "2d41e3e6-5757-4954-ad18-681e5976f0cf",
        "metadata": {}
      },
      {
        "path": ".llmstruct_index/orchestration",
        "type": "directory",
        "artifact_id": "f033a426-a494-4edd-bd4a-d29eddf9dce4",
        "metadata": {}
      },
      {
        "path": ".llmstruct_index/orchestration/core",
        "type": "directory",
        "artifact_id": "84e89e92-f9a6-4cf2-a549-9a1d40a393fb",
        "metadata": {}
      },
      {
        "path": ".llmstruct_index/orchestration/executors",
        "type": "directory",
        "artifact_id": "e12ecdb3-a3e3-4c31-8ba6-b85724c0663e",
        "metadata": {}
      },
      {
        "path": ".llmstruct_index/orchestration/integration",
        "type": "directory",
        "artifact_id": "895bfbf6-a954-4b1d-b9ce-c2bc94ba11df",
        "metadata": {}
      },
      {
        "path": "integration",
        "type": "directory",
        "artifact_id": "9a013959-6e4d-485d-832e-c02d0807298c",
        "metadata": {}
      },
      {
        "path": "orchestration",
        "type": "directory",
        "artifact_id": "953870b4-5667-4562-a03f-858b8fecf02b",
        "metadata": {}
      },
      {
        "path": "orchestration/__init__.py",
        "type": "file",
        "artifact_id": "651083eb-bc23-44d4-bf02-7b51bd4b5eb9",
        "metadata": {}
      },
      {
        "path": "orchestration/core",
        "type": "directory",
        "artifact_id": "1ad85c6e-fd59-47b2-91da-b5dfb7b04d40",
        "metadata": {}
      },
      {
        "path": "orchestration/core/__init__.py",
        "type": "file",
        "artifact_id": "f2dc7960-5e07-4417-9afe-36d1c9bd66fb",
        "metadata": {}
      },
      {
        "path": "orchestration/core/test_execution_modes.py",
        "type": "file",
        "artifact_id": "796b6122-8060-4255-9b0e-56a2ceae0bd6",
        "metadata": {}
      },
      {
        "path": "orchestration/core/test_task_models.py",
        "type": "file",
        "artifact_id": "5d7f691c-de86-4b0c-9f97-46ad12118769",
        "metadata": {}
      },
      {
        "path": "orchestration/executors",
        "type": "directory",
        "artifact_id": "c2d76efe-7a46-4d03-ad0c-5f8896ed5e08",
        "metadata": {}
      },
      {
        "path": "orchestration/executors/__init__.py",
        "type": "file",
        "artifact_id": "72060942-4ca0-4cc8-a81d-c9cce1dd19ae",
        "metadata": {}
      },
      {
        "path": "orchestration/executors/test_parallel_executor.py",
        "type": "file",
        "artifact_id": "ceb6cb1a-0055-4441-bab8-92d57508f212",
        "metadata": {}
      },
      {
        "path": "orchestration/fixtures.py",
        "type": "file",
        "artifact_id": "5bc700f7-2cf4-4672-b6fd-a0caa7b694d1",
        "metadata": {}
      },
      {
        "path": "orchestration/integration",
        "type": "directory",
        "artifact_id": "c720a920-2314-4c1a-ba3f-e3c4b8eb5fe9",
        "metadata": {}
      },
      {
        "path": "orchestration/integration/__init__.py",
        "type": "file",
        "artifact_id": "9debb391-dcc9-4ebe-a616-3c883bc870c5",
        "metadata": {}
      },
      {
        "path": "orchestration/integration/test_epic5_components.py",
        "type": "file",
        "artifact_id": "8a6eac3c-e280-41e4-975a-e0315738a44f",
        "metadata": {}
      },
      {
        "path": "test_api.py",
        "type": "file",
        "artifact_id": "ce64a1f4-ae31-4277-b9da-0678dcf66dc7",
        "metadata": {}
      },
      {
        "path": "test_cli.py",
        "type": "file",
        "artifact_id": "a019cf3f-e164-474f-8727-d87ad3cf4e57",
        "metadata": {}
      },
      {
        "path": "test_ollama_function_calling.py",
        "type": "file",
        "artifact_id": "93b7ea4b-a039-4db7-b2c5-1aa4bcd89b95",
        "metadata": {}
      },
      {
        "path": "test_self_refine_pipeline.py",
        "type": "file",
        "artifact_id": "22cd90aa-5744-4fd5-9bc1-f5de0e81018f",
        "metadata": {}
      },
      {
        "path": "test_smoke.py",
        "type": "file",
        "artifact_id": "d256335e-2064-4738-b153-b501770fe2b2",
        "metadata": {}
      },
      {
        "path": "test_task_router.py",
        "type": "file",
        "artifact_id": "3f548e9c-515b-4009-b26e-2847bf567ee1",
        "metadata": {}
      }
    ]
  },
  "toc": [
    {
      "module_id": "test_task_router",
      "path": "test_task_router.py",
      "category": "test",
      "functions": 35,
      "classes": 5,
      "summary": "Comprehensive tests for Epic 5 TaskRouter implementation",
      "artifact_id": "15fd7652-48d0-418a-a556-9ace93708e64"
    },
    {
      "module_id": "test_ollama_function_calling",
      "path": "test_ollama_function_calling.py",
      "category": "test",
      "functions": 5,
      "classes": 0,
      "summary": "Test script for Ollama function calling capability",
      "artifact_id": "120fb09f-ffaa-44ba-84f9-3c79355a1358"
    },
    {
      "module_id": "test_api",
      "path": "test_api.py",
      "category": "test",
      "functions": 4,
      "classes": 0,
      "summary": "Tests for llmgenie FastAPI application",
      "artifact_id": "8a650a08-3499-40f2-b5c8-0155611bd4f9"
    },
    {
      "module_id": "test_cli",
      "path": "test_cli.py",
      "category": "test",
      "functions": 2,
      "classes": 0,
      "summary": "Tests for llmgenie CLI module",
      "artifact_id": "cff756a0-c9d8-4660-ba4b-f58f7f3f4a56"
    },
    {
      "module_id": "test_self_refine_pipeline",
      "path": "test_self_refine_pipeline.py",
      "category": "test",
      "functions": 20,
      "classes": 3,
      "summary": "Tests for Self-Refine Pipeline System",
      "artifact_id": "c8842be3-0e69-455d-8f46-6f1e638132d1"
    },
    {
      "module_id": "test_smoke",
      "path": "test_smoke.py",
      "category": "test",
      "functions": 1,
      "classes": 0,
      "summary": "",
      "artifact_id": "cd5aad59-c68e-4406-98f5-b5890df05a9d"
    },
    {
      "module_id": "orchestration.__init__",
      "path": "orchestration/__init__.py",
      "category": "test",
      "functions": 0,
      "classes": 0,
      "summary": "Modular tests for Multi-Agent Orchestration",
      "artifact_id": "29d3296f-f29e-4fda-b7a6-7dd762899fb1"
    },
    {
      "module_id": "orchestration.fixtures",
      "path": "orchestration/fixtures.py",
      "category": "test",
      "functions": 4,
      "classes": 1,
      "summary": "Shared test fixtures for orchestration testing",
      "artifact_id": "45ee0175-ad13-45c7-9a08-cb679d046b75"
    },
    {
      "module_id": "orchestration.integration.__init__",
      "path": "orchestration/integration/__init__.py",
      "category": "test",
      "functions": 0,
      "classes": 0,
      "summary": "Integration tests",
      "artifact_id": "b0c10d66-06ad-409e-89b1-1483224f5174"
    },
    {
      "module_id": "orchestration.integration.test_epic5_components",
      "path": "orchestration/integration/test_epic5_components.py",
      "category": "test",
      "functions": 5,
      "classes": 1,
      "summary": "Test integration with Epic 5 components",
      "artifact_id": "77deb1cc-4694-40a6-84f9-f51984399b97"
    },
    {
      "module_id": "orchestration.core.test_task_models",
      "path": "orchestration/core/test_task_models.py",
      "category": "test",
      "functions": 8,
      "classes": 2,
      "summary": "Test OrchestrationTask and OrchestrationResult models",
      "artifact_id": "6f965c64-50b4-4a91-b30a-bea3f3023a4d"
    },
    {
      "module_id": "orchestration.core.__init__",
      "path": "orchestration/core/__init__.py",
      "category": "test",
      "functions": 0,
      "classes": 0,
      "summary": "Core orchestration component tests",
      "artifact_id": "57c6266f-d965-4246-a13b-5dd8db2f8750"
    },
    {
      "module_id": "orchestration.core.test_execution_modes",
      "path": "orchestration/core/test_execution_modes.py",
      "category": "test",
      "functions": 7,
      "classes": 1,
      "summary": "Test ExecutionMode enum and smart suggestions",
      "artifact_id": "92e14f2d-ff8d-4284-ae7b-d3c7516e02cd"
    },
    {
      "module_id": "orchestration.executors.test_parallel_executor",
      "path": "orchestration/executors/test_parallel_executor.py",
      "category": "test",
      "functions": 6,
      "classes": 1,
      "summary": "Test ParallelExecutor implementation",
      "artifact_id": "ba367d41-da53-49ee-8d39-941b93c5ef12"
    },
    {
      "module_id": "orchestration.executors.__init__",
      "path": "orchestration/executors/__init__.py",
      "category": "test",
      "functions": 0,
      "classes": 0,
      "summary": "Executor strategy tests",
      "artifact_id": "a0333ef7-1bb8-4411-a1e4-97e752d39d8b"
    }
  ],
  "modules": [
    {
      "module_id": "test_task_router",
      "path": "test_task_router.py",
      "category": "test",
      "module_doc": "Comprehensive tests for Epic 5 TaskRouter implementation\n\nTests task classification, model routing, and integration with FastAPI",
      "functions": [
        {
          "name": "setup_method",
          "docstring": "Setup classifier for each test",
          "line_range": [
            20,
            22
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_code_generation_classification",
          "docstring": "Test code generation task classification",
          "line_range": [
            24,
            31
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_documentation_classification",
          "docstring": "Test documentation task classification",
          "line_range": [
            33,
            39
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_complex_reasoning_classification",
          "docstring": "Test complex reasoning task classification",
          "line_range": [
            41,
            48
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_debugging_classification",
          "docstring": "Test debugging task classification",
          "line_range": [
            50,
            56
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_complexity_scoring",
          "docstring": "Test complexity level calculation",
          "line_range": [
            58,
            73
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_context_based_classification",
          "docstring": "Test classification with context information",
          "line_range": [
            75,
            86
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "setup_method",
          "docstring": "Setup router for each test",
          "line_range": [
            92,
            95
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_ollama_routing_preference",
          "docstring": "Test routing to Ollama for code generation",
          "line_range": [
            98,
            106
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_claude_routing_preference",
          "docstring": "Test routing to Claude for complex reasoning",
          "line_range": [
            109,
            116
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_user_model_preference",
          "docstring": "Test user model preference override",
          "line_range": [
            119,
            130
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_fallback_model_selection",
          "docstring": "Test fallback model logic",
          "line_range": [
            133,
            140
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_ollama_execution",
          "docstring": "Test actual Ollama execution",
          "line_range": [
            144,
            162
          ],
          "parameters": [
            "self",
            "mock_post"
          ],
          "decorators": [
            "pytest.mark.asyncio",
            "patch('httpx.AsyncClient.post')"
          ]
        },
        {
          "name": "test_claude_execution_placeholder",
          "docstring": "Test Claude execution (placeholder implementation)",
          "line_range": [
            165,
            174
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_error_handling",
          "docstring": "Test error handling in execution",
          "line_range": [
            178,
            190
          ],
          "parameters": [
            "self",
            "mock_post"
          ],
          "decorators": [
            "pytest.mark.asyncio",
            "patch('httpx.AsyncClient.post')"
          ]
        },
        {
          "name": "setup_method",
          "docstring": "Setup for performance tests",
          "line_range": [
            196,
            198
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_model_performance_baselines",
          "docstring": "Test that performance baselines match Epic 5 findings",
          "line_range": [
            200,
            210
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_routing_decision_optimization",
          "docstring": "Test routing optimization for performance",
          "line_range": [
            213,
            221
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_quality_threshold_calculation",
          "docstring": "Test quality threshold based on task complexity",
          "line_range": [
            223,
            234
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_agent_request_compatibility",
          "docstring": "Test compatibility with existing AgentRequest model",
          "line_range": [
            240,
            260
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_routing_decision_serialization",
          "docstring": "Test that RoutingDecision can be serialized for API responses",
          "line_range": [
            263,
            279
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "setup_method",
          "docstring": "Setup for quality validator tests",
          "line_range": [
            285,
            287
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_python_code_validation_success",
          "docstring": "Test successful Python code validation",
          "line_range": [
            289,
            311
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_python_code_validation_syntax_error",
          "docstring": "Test Python code with syntax errors",
          "line_range": [
            313,
            326
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_javascript_code_validation",
          "docstring": "Test JavaScript code validation",
          "line_range": [
            328,
            349
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_text_validation_high_quality",
          "docstring": "Test high-quality text validation",
          "line_range": [
            351,
            374
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_text_validation_poor_quality",
          "docstring": "Test poor quality text validation",
          "line_range": [
            376,
            385
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_documentation_specific_validation",
          "docstring": "Test documentation-specific validation",
          "line_range": [
            387,
            411
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_fallback_decision_making",
          "docstring": "Test fallback decision based on task type and quality",
          "line_range": [
            413,
            455
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_quality_thresholds_by_task_type",
          "docstring": "Test different quality thresholds for different task types",
          "line_range": [
            457,
            466
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_coherence_score_calculation",
          "docstring": "Test text coherence scoring",
          "line_range": [
            468,
            481
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_completeness_score_calculation",
          "docstring": "Test text completeness scoring",
          "line_range": [
            483,
            499
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_empty_input_handling",
          "docstring": "Test handling of empty inputs",
          "line_range": [
            501,
            513
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_quality_metrics_extraction",
          "docstring": "Test quality metrics extraction for monitoring",
          "line_range": [
            515,
            532
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_generic_code_validation",
          "docstring": "Test generic code validation for unknown languages",
          "line_range": [
            534,
            547
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "TestTaskClassifier",
          "docstring": "Test TaskClassifier with Epic 5 research patterns",
          "line_range": [
            17,
            86
          ],
          "methods": [
            {
              "name": "setup_method",
              "docstring": "Setup classifier for each test",
              "line_range": [
                20,
                22
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_code_generation_classification",
              "docstring": "Test code generation task classification",
              "line_range": [
                24,
                31
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_documentation_classification",
              "docstring": "Test documentation task classification",
              "line_range": [
                33,
                39
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_complex_reasoning_classification",
              "docstring": "Test complex reasoning task classification",
              "line_range": [
                41,
                48
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_debugging_classification",
              "docstring": "Test debugging task classification",
              "line_range": [
                50,
                56
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_complexity_scoring",
              "docstring": "Test complexity level calculation",
              "line_range": [
                58,
                73
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_context_based_classification",
              "docstring": "Test classification with context information",
              "line_range": [
                75,
                86
              ],
              "parameters": [
                "self"
              ]
            }
          ],
          "bases": []
        },
        {
          "name": "TestModelRouter",
          "docstring": "Test ModelRouter with Epic 5 integration patterns",
          "line_range": [
            89,
            190
          ],
          "methods": [
            {
              "name": "setup_method",
              "docstring": "Setup router for each test",
              "line_range": [
                92,
                95
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_ollama_routing_preference",
              "docstring": "Test routing to Ollama for code generation",
              "line_range": [
                98,
                106
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_claude_routing_preference",
              "docstring": "Test routing to Claude for complex reasoning",
              "line_range": [
                109,
                116
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_user_model_preference",
              "docstring": "Test user model preference override",
              "line_range": [
                119,
                130
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_fallback_model_selection",
              "docstring": "Test fallback model logic",
              "line_range": [
                133,
                140
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_ollama_execution",
              "docstring": "Test actual Ollama execution",
              "line_range": [
                144,
                162
              ],
              "parameters": [
                "self",
                "mock_post"
              ]
            },
            {
              "name": "test_claude_execution_placeholder",
              "docstring": "Test Claude execution (placeholder implementation)",
              "line_range": [
                165,
                174
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_error_handling",
              "docstring": "Test error handling in execution",
              "line_range": [
                178,
                190
              ],
              "parameters": [
                "self",
                "mock_post"
              ]
            }
          ],
          "bases": []
        },
        {
          "name": "TestPerformanceOptimization",
          "docstring": "Test performance optimization based on Epic 5 baselines",
          "line_range": [
            193,
            234
          ],
          "methods": [
            {
              "name": "setup_method",
              "docstring": "Setup for performance tests",
              "line_range": [
                196,
                198
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_model_performance_baselines",
              "docstring": "Test that performance baselines match Epic 5 findings",
              "line_range": [
                200,
                210
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_routing_decision_optimization",
              "docstring": "Test routing optimization for performance",
              "line_range": [
                213,
                221
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_quality_threshold_calculation",
              "docstring": "Test quality threshold based on task complexity",
              "line_range": [
                223,
                234
              ],
              "parameters": [
                "self"
              ]
            }
          ],
          "bases": []
        },
        {
          "name": "TestIntegrationWithFastAPI",
          "docstring": "Test integration with existing FastAPI infrastructure",
          "line_range": [
            237,
            279
          ],
          "methods": [
            {
              "name": "test_agent_request_compatibility",
              "docstring": "Test compatibility with existing AgentRequest model",
              "line_range": [
                240,
                260
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_routing_decision_serialization",
              "docstring": "Test that RoutingDecision can be serialized for API responses",
              "line_range": [
                263,
                279
              ],
              "parameters": [
                "self"
              ]
            }
          ],
          "bases": []
        },
        {
          "name": "TestQualityValidator",
          "docstring": "Test enhanced Quality Validator with real validation logic",
          "line_range": [
            282,
            547
          ],
          "methods": [
            {
              "name": "setup_method",
              "docstring": "Setup for quality validator tests",
              "line_range": [
                285,
                287
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_python_code_validation_success",
              "docstring": "Test successful Python code validation",
              "line_range": [
                289,
                311
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_python_code_validation_syntax_error",
              "docstring": "Test Python code with syntax errors",
              "line_range": [
                313,
                326
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_javascript_code_validation",
              "docstring": "Test JavaScript code validation",
              "line_range": [
                328,
                349
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_text_validation_high_quality",
              "docstring": "Test high-quality text validation",
              "line_range": [
                351,
                374
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_text_validation_poor_quality",
              "docstring": "Test poor quality text validation",
              "line_range": [
                376,
                385
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_documentation_specific_validation",
              "docstring": "Test documentation-specific validation",
              "line_range": [
                387,
                411
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_fallback_decision_making",
              "docstring": "Test fallback decision based on task type and quality",
              "line_range": [
                413,
                455
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_quality_thresholds_by_task_type",
              "docstring": "Test different quality thresholds for different task types",
              "line_range": [
                457,
                466
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_coherence_score_calculation",
              "docstring": "Test text coherence scoring",
              "line_range": [
                468,
                481
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_completeness_score_calculation",
              "docstring": "Test text completeness scoring",
              "line_range": [
                483,
                499
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_empty_input_handling",
              "docstring": "Test handling of empty inputs",
              "line_range": [
                501,
                513
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_quality_metrics_extraction",
              "docstring": "Test quality metrics extraction for monitoring",
              "line_range": [
                515,
                532
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_generic_code_validation",
              "docstring": "Test generic code validation for unknown languages",
              "line_range": [
                534,
                547
              ],
              "parameters": [
                "self"
              ]
            }
          ],
          "bases": []
        }
      ],
      "callgraph": {
        "setup_method": [
          "QualityValidator"
        ],
        "test_code_generation_classification": [],
        "test_documentation_classification": [],
        "test_complex_reasoning_classification": [],
        "test_debugging_classification": [],
        "test_complexity_scoring": [],
        "test_context_based_classification": [],
        "test_ollama_routing_preference": [],
        "test_claude_routing_preference": [],
        "test_user_model_preference": [],
        "test_fallback_model_selection": [],
        "test_ollama_execution": [
          "patch",
          "Mock"
        ],
        "test_claude_execution_placeholder": [],
        "test_error_handling": [
          "patch",
          "Exception"
        ],
        "test_model_performance_baselines": [
          "expected_baselines.items"
        ],
        "test_routing_decision_optimization": [],
        "test_quality_threshold_calculation": [
          "TaskClassifier",
          "classifier.classify_task"
        ],
        "test_agent_request_compatibility": [
          "isinstance",
          "TaskClassifier",
          "classifier.classify_task"
        ],
        "test_routing_decision_serialization": [
          "ModelRouter",
          "isinstance",
          "router.route_task"
        ],
        "test_python_code_validation_success": [],
        "test_python_code_validation_syntax_error": [
          "any",
          "issue.lower"
        ],
        "test_javascript_code_validation": [],
        "test_text_validation_high_quality": [],
        "test_text_validation_poor_quality": [],
        "test_documentation_specific_validation": [
          "len"
        ],
        "test_fallback_decision_making": [
          "QualityResult"
        ],
        "test_quality_thresholds_by_task_type": [],
        "test_coherence_score_calculation": [],
        "test_completeness_score_calculation": [
          "complete_text.lower"
        ],
        "test_empty_input_handling": [],
        "test_quality_metrics_extraction": [
          "QualityResult"
        ],
        "test_generic_code_validation": []
      },
      "dependencies": [
        "AsyncMock",
        "ComplexityLevel",
        "Mock",
        "ModelChoice",
        "ModelRouter",
        "QualityResult",
        "QualityScore",
        "QualityValidator",
        "RoutingDecision",
        "TaskClassifier",
        "TaskType",
        "asyncio",
        "patch",
        "pytest",
        "src.llmgenie.task_router",
        "src.llmgenie.task_router.task_classifier",
        "unittest.mock"
      ],
      "hash": "07b8ae843db69e19353cb6bd830e348ff42e34847c774212be84ff248f3dd137",
      "artifact_id": "15fd7652-48d0-418a-a556-9ace93708e64"
    },
    {
      "module_id": "test_ollama_function_calling",
      "path": "test_ollama_function_calling.py",
      "category": "test",
      "module_doc": "Test script for Ollama function calling capability\nPart of Epic 5: MCP-Ollama Integration\n\nThis script tests:\n1. Basic function calling with different models\n2. Performance measurement\n3. Quality validation",
      "functions": [
        {
          "name": "get_current_weather",
          "docstring": "Get the current weather for a city (mock function for testing)\n\nArgs:\n    city: The name of the city\n    \nReturns:\n    dict: Weather information",
          "line_range": [
            19,
            37
          ],
          "parameters": [
            "city"
          ],
          "decorators": []
        },
        {
          "name": "add_two_numbers",
          "docstring": "Add two numbers together\n\nArgs:\n    a: The first integer number\n    b: The second integer number\n    \nReturns:\n    int: The sum of the two numbers",
          "line_range": [
            40,
            51
          ],
          "parameters": [
            "a",
            "b"
          ],
          "decorators": []
        },
        {
          "name": "test_model_function_calling",
          "docstring": "Test function calling capability for a specific model\n\nArgs:\n    model_name: Name of the Ollama model to test\n    \nReturns:\n    dict: Test results including performance metrics",
          "line_range": [
            54,
            128
          ],
          "parameters": [
            "model_name"
          ],
          "decorators": []
        },
        {
          "name": "test_openai_compatible_endpoint",
          "docstring": "Test OpenAI compatible endpoint functionality\n\nReturns:\n    dict: Test results for OpenAI compatibility",
          "line_range": [
            131,
            178
          ],
          "parameters": [],
          "decorators": []
        },
        {
          "name": "main",
          "docstring": "Main test execution function",
          "line_range": [
            181,
            265
          ],
          "parameters": [],
          "decorators": []
        }
      ],
      "classes": [],
      "callgraph": {
        "get_current_weather": [
          "weather_data.get"
        ],
        "add_two_numbers": [],
        "test_model_function_calling": [
          "add_two_numbers",
          "ollama.chat",
          "time.time",
          "len",
          "print",
          "str",
          "get_current_weather"
        ],
        "test_openai_compatible_endpoint": [
          "response.json",
          "time.time",
          "requests.post",
          "data.get",
          "print",
          "str",
          "len"
        ],
        "main": [
          "hasattr",
          "sum",
          "test_openai_compatible_endpoint",
          "ollama.list",
          "open",
          "len",
          "result.get",
          "isinstance",
          "print",
          "test_results.items",
          "json.dump",
          "test_model_function_calling"
        ]
      },
      "dependencies": [
        "Any",
        "Dict",
        "List",
        "json",
        "ollama",
        "requests",
        "time",
        "typing"
      ],
      "hash": "ee0ccf1cf8a278a300a6c166371f0df26dd9bc8843a50e0fbda21bd40c742b25",
      "artifact_id": "120fb09f-ffaa-44ba-84f9-3c79355a1358"
    },
    {
      "module_id": "test_api",
      "path": "test_api.py",
      "category": "test",
      "module_doc": "Tests for llmgenie FastAPI application",
      "functions": [
        {
          "name": "test_health_check",
          "docstring": "Test health check endpoint",
          "line_range": [
            17,
            24
          ],
          "parameters": [],
          "decorators": []
        },
        {
          "name": "test_workflow_modes",
          "docstring": "Test workflow modes endpoint",
          "line_range": [
            26,
            34
          ],
          "parameters": [],
          "decorators": []
        },
        {
          "name": "test_agent_execute",
          "docstring": "Test agent execution endpoint",
          "line_range": [
            36,
            48
          ],
          "parameters": [],
          "decorators": []
        },
        {
          "name": "test_mcp_tools_placeholder",
          "docstring": "Test MCP tools placeholder endpoint",
          "line_range": [
            50,
            56
          ],
          "parameters": [],
          "decorators": []
        }
      ],
      "classes": [],
      "callgraph": {
        "test_health_check": [
          "client.get",
          "response.json"
        ],
        "test_workflow_modes": [
          "client.get",
          "response.json"
        ],
        "test_agent_execute": [
          "client.post",
          "response.json"
        ],
        "test_mcp_tools_placeholder": [
          "client.post",
          "response.json"
        ]
      },
      "dependencies": [
        "TestClient",
        "app",
        "fastapi.testclient",
        "llmgenie.api.main",
        "os",
        "pytest",
        "sys"
      ],
      "hash": "1a2b1fd51aa2161c4ce2e92b7ab2b226923863ee708de205c59d35e5fbd83131",
      "artifact_id": "8a650a08-3499-40f2-b5c8-0155611bd4f9"
    },
    {
      "module_id": "test_cli",
      "path": "test_cli.py",
      "category": "test",
      "module_doc": "Tests for llmgenie CLI module",
      "functions": [
        {
          "name": "test_cli_import",
          "docstring": "Test that CLI module can be imported",
          "line_range": [
            12,
            18
          ],
          "parameters": [],
          "decorators": []
        },
        {
          "name": "test_llm_client_import",
          "docstring": "Test that LLM client can be imported",
          "line_range": [
            20,
            26
          ],
          "parameters": [],
          "decorators": []
        }
      ],
      "classes": [],
      "callgraph": {
        "test_cli_import": [
          "pytest.fail"
        ],
        "test_llm_client_import": [
          "pytest.fail"
        ]
      },
      "dependencies": [
        "cli",
        "llm_client",
        "llmgenie",
        "os",
        "pytest",
        "sys"
      ],
      "hash": "892f36ff6a9ab5c3fdb8215906c8d9ca50a2fd5fbe714e8475dfd92bafef5fef",
      "artifact_id": "cff756a0-c9d8-4660-ba4b-f58f7f3f4a56"
    },
    {
      "module_id": "test_self_refine_pipeline",
      "path": "test_self_refine_pipeline.py",
      "category": "test",
      "module_doc": "Tests for Self-Refine Pipeline System\nComprehensive testing of MCP-enhanced iterative improvement",
      "functions": [
        {
          "name": "pipeline",
          "docstring": "Create a test pipeline instance",
          "line_range": [
            24,
            26
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.fixture"
          ]
        },
        {
          "name": "sample_code",
          "docstring": "Sample code for testing",
          "line_range": [
            29,
            40
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.fixture"
          ]
        },
        {
          "name": "sample_text",
          "docstring": "Sample text for testing",
          "line_range": [
            43,
            45
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.fixture"
          ]
        },
        {
          "name": "test_pipeline_initialization",
          "docstring": "Test pipeline initialization",
          "line_range": [
            47,
            51
          ],
          "parameters": [
            "self",
            "pipeline"
          ],
          "decorators": []
        },
        {
          "name": "test_refine_code",
          "docstring": "Test code refinement",
          "line_range": [
            53,
            61
          ],
          "parameters": [
            "self",
            "pipeline",
            "sample_code"
          ],
          "decorators": []
        },
        {
          "name": "test_refine_text",
          "docstring": "Test text refinement",
          "line_range": [
            63,
            69
          ],
          "parameters": [
            "self",
            "pipeline",
            "sample_text"
          ],
          "decorators": []
        },
        {
          "name": "test_confidence_threshold_reached",
          "docstring": "Test that refinement stops when confidence threshold is reached",
          "line_range": [
            72,
            82
          ],
          "parameters": [
            "self",
            "pipeline",
            "sample_code"
          ],
          "decorators": []
        },
        {
          "name": "test_max_iterations_limit",
          "docstring": "Test that refinement respects max iterations limit",
          "line_range": [
            84,
            93
          ],
          "parameters": [
            "self",
            "pipeline",
            "sample_code"
          ],
          "decorators": []
        },
        {
          "name": "test_mcp_tools_integration",
          "docstring": "Test MCP tools integration in refinement process",
          "line_range": [
            95,
            104
          ],
          "parameters": [
            "self",
            "pipeline",
            "sample_code"
          ],
          "decorators": []
        },
        {
          "name": "test_refine_code_file",
          "docstring": "Test refinement of entire code file",
          "line_range": [
            106,
            128
          ],
          "parameters": [
            "self",
            "pipeline",
            "sample_code"
          ],
          "decorators": []
        },
        {
          "name": "test_refinement_report_generation",
          "docstring": "Test refinement report generation",
          "line_range": [
            130,
            148
          ],
          "parameters": [
            "self",
            "pipeline",
            "sample_code"
          ],
          "decorators": []
        },
        {
          "name": "test_quick_refine_functions",
          "docstring": "Test convenience quick refine functions",
          "line_range": [
            150,
            159
          ],
          "parameters": [
            "self",
            "sample_code",
            "sample_text"
          ],
          "decorators": []
        },
        {
          "name": "test_enhancement_context_building",
          "docstring": "Test context enhancement with MCP tools",
          "line_range": [
            161,
            171
          ],
          "parameters": [
            "self",
            "pipeline"
          ],
          "decorators": []
        },
        {
          "name": "test_critique_generation",
          "docstring": "Test critique generation with MCP integration",
          "line_range": [
            173,
            186
          ],
          "parameters": [
            "self",
            "pipeline"
          ],
          "decorators": []
        },
        {
          "name": "test_validation_logic",
          "docstring": "Test improvement validation logic",
          "line_range": [
            188,
            203
          ],
          "parameters": [
            "self",
            "pipeline"
          ],
          "decorators": []
        },
        {
          "name": "test_error_handling_file_not_found",
          "docstring": "Test error handling for non-existent files",
          "line_range": [
            205,
            208
          ],
          "parameters": [
            "self",
            "pipeline"
          ],
          "decorators": []
        },
        {
          "name": "test_empty_results_report",
          "docstring": "Test report generation with empty results",
          "line_range": [
            210,
            214
          ],
          "parameters": [
            "self",
            "pipeline"
          ],
          "decorators": []
        },
        {
          "name": "test_auto_logging_integration",
          "docstring": "Test integration with auto-logging system",
          "line_range": [
            218,
            231
          ],
          "parameters": [
            "self",
            "mock_auto_logger",
            "pipeline",
            "sample_code"
          ],
          "decorators": [
            "patch('src.rag_context.interfaces.self_refine_pipeline.AUTO_LOGGING_AVAILABLE', True)",
            "patch('src.rag_context.interfaces.self_refine_pipeline.auto_logger')"
          ]
        },
        {
          "name": "test_all_refinement_types",
          "docstring": "Test that all refinement types are properly defined",
          "line_range": [
            237,
            243
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_refinement_result_creation",
          "docstring": "Test RefinementResult creation and attributes",
          "line_range": [
            249,
            269
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "TestSelfRefinePipeline",
          "docstring": "Test suite for Self-Refine Pipeline",
          "line_range": [
            20,
            231
          ],
          "methods": [
            {
              "name": "pipeline",
              "docstring": "Create a test pipeline instance",
              "line_range": [
                24,
                26
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "sample_code",
              "docstring": "Sample code for testing",
              "line_range": [
                29,
                40
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "sample_text",
              "docstring": "Sample text for testing",
              "line_range": [
                43,
                45
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_pipeline_initialization",
              "docstring": "Test pipeline initialization",
              "line_range": [
                47,
                51
              ],
              "parameters": [
                "self",
                "pipeline"
              ]
            },
            {
              "name": "test_refine_code",
              "docstring": "Test code refinement",
              "line_range": [
                53,
                61
              ],
              "parameters": [
                "self",
                "pipeline",
                "sample_code"
              ]
            },
            {
              "name": "test_refine_text",
              "docstring": "Test text refinement",
              "line_range": [
                63,
                69
              ],
              "parameters": [
                "self",
                "pipeline",
                "sample_text"
              ]
            },
            {
              "name": "test_confidence_threshold_reached",
              "docstring": "Test that refinement stops when confidence threshold is reached",
              "line_range": [
                72,
                82
              ],
              "parameters": [
                "self",
                "pipeline",
                "sample_code"
              ]
            },
            {
              "name": "test_max_iterations_limit",
              "docstring": "Test that refinement respects max iterations limit",
              "line_range": [
                84,
                93
              ],
              "parameters": [
                "self",
                "pipeline",
                "sample_code"
              ]
            },
            {
              "name": "test_mcp_tools_integration",
              "docstring": "Test MCP tools integration in refinement process",
              "line_range": [
                95,
                104
              ],
              "parameters": [
                "self",
                "pipeline",
                "sample_code"
              ]
            },
            {
              "name": "test_refine_code_file",
              "docstring": "Test refinement of entire code file",
              "line_range": [
                106,
                128
              ],
              "parameters": [
                "self",
                "pipeline",
                "sample_code"
              ]
            },
            {
              "name": "test_refinement_report_generation",
              "docstring": "Test refinement report generation",
              "line_range": [
                130,
                148
              ],
              "parameters": [
                "self",
                "pipeline",
                "sample_code"
              ]
            },
            {
              "name": "test_quick_refine_functions",
              "docstring": "Test convenience quick refine functions",
              "line_range": [
                150,
                159
              ],
              "parameters": [
                "self",
                "sample_code",
                "sample_text"
              ]
            },
            {
              "name": "test_enhancement_context_building",
              "docstring": "Test context enhancement with MCP tools",
              "line_range": [
                161,
                171
              ],
              "parameters": [
                "self",
                "pipeline"
              ]
            },
            {
              "name": "test_critique_generation",
              "docstring": "Test critique generation with MCP integration",
              "line_range": [
                173,
                186
              ],
              "parameters": [
                "self",
                "pipeline"
              ]
            },
            {
              "name": "test_validation_logic",
              "docstring": "Test improvement validation logic",
              "line_range": [
                188,
                203
              ],
              "parameters": [
                "self",
                "pipeline"
              ]
            },
            {
              "name": "test_error_handling_file_not_found",
              "docstring": "Test error handling for non-existent files",
              "line_range": [
                205,
                208
              ],
              "parameters": [
                "self",
                "pipeline"
              ]
            },
            {
              "name": "test_empty_results_report",
              "docstring": "Test report generation with empty results",
              "line_range": [
                210,
                214
              ],
              "parameters": [
                "self",
                "pipeline"
              ]
            },
            {
              "name": "test_auto_logging_integration",
              "docstring": "Test integration with auto-logging system",
              "line_range": [
                218,
                231
              ],
              "parameters": [
                "self",
                "mock_auto_logger",
                "pipeline",
                "sample_code"
              ]
            }
          ],
          "bases": []
        },
        {
          "name": "TestRefinementTypes",
          "docstring": "Test different refinement types",
          "line_range": [
            234,
            243
          ],
          "methods": [
            {
              "name": "test_all_refinement_types",
              "docstring": "Test that all refinement types are properly defined",
              "line_range": [
                237,
                243
              ],
              "parameters": [
                "self"
              ]
            }
          ],
          "bases": []
        },
        {
          "name": "TestRefinementResult",
          "docstring": "Test RefinementResult dataclass",
          "line_range": [
            246,
            269
          ],
          "methods": [
            {
              "name": "test_refinement_result_creation",
              "docstring": "Test RefinementResult creation and attributes",
              "line_range": [
                249,
                269
              ],
              "parameters": [
                "self"
              ]
            }
          ],
          "bases": []
        }
      ],
      "callgraph": {
        "pipeline": [
          "SelfRefinePipeline"
        ],
        "sample_code": [],
        "sample_text": [],
        "test_pipeline_initialization": [
          "isinstance"
        ],
        "test_refine_code": [
          "all",
          "isinstance",
          "len",
          "pipeline.refine"
        ],
        "test_refine_text": [
          "len",
          "pipeline.refine"
        ],
        "test_confidence_threshold_reached": [
          "len",
          "patch.object",
          "pipeline.refine"
        ],
        "test_max_iterations_limit": [
          "len",
          "patch.object",
          "pipeline.refine"
        ],
        "test_mcp_tools_integration": [
          "isinstance",
          "len",
          "pipeline.refine"
        ],
        "test_refine_code_file": [
          "backup_file.unlink",
          "backup_file.exists",
          "temp_file.read_text",
          "pipeline.refine_code_file",
          "len",
          "temp_file.unlink",
          "Path",
          "temp_file.with_suffix",
          "tempfile.NamedTemporaryFile",
          "str",
          "backup_file.read_text",
          "f.write"
        ],
        "test_refinement_report_generation": [
          "pipeline.generate_refinement_report",
          "len",
          "pipeline.refine"
        ],
        "test_quick_refine_functions": [
          "quick_refine_text",
          "isinstance",
          "len",
          "quick_refine_code"
        ],
        "test_enhancement_context_building": [
          "pipeline._enhance_context",
          "len"
        ],
        "test_critique_generation": [
          "pipeline._critique_with_mcp",
          "isinstance",
          "len"
        ],
        "test_validation_logic": [
          "pipeline._validate_improvements"
        ],
        "test_error_handling_file_not_found": [
          "pipeline.refine_code_file",
          "pytest.raises"
        ],
        "test_empty_results_report": [
          "pipeline.generate_refinement_report"
        ],
        "test_auto_logging_integration": [
          "patch",
          "len",
          "pipeline.refine"
        ],
        "test_all_refinement_types": [],
        "test_refinement_result_creation": [
          "RefinementResult"
        ]
      },
      "dependencies": [
        "Mock",
        "Path",
        "RefinementResult",
        "RefinementType",
        "SelfRefinePipeline",
        "patch",
        "pathlib",
        "pytest",
        "quick_refine_code",
        "quick_refine_text",
        "src.rag_context.interfaces.self_refine_pipeline",
        "tempfile",
        "unittest.mock"
      ],
      "hash": "05a17d4148ab1b8353ab668170382fc22d156d6746c10cd6bb09241703f4f0c8",
      "artifact_id": "c8842be3-0e69-455d-8f46-6f1e638132d1"
    },
    {
      "module_id": "test_smoke",
      "path": "test_smoke.py",
      "category": "test",
      "module_doc": "",
      "functions": [
        {
          "name": "test_smoke",
          "docstring": "",
          "line_range": [
            1,
            2
          ],
          "parameters": [],
          "decorators": []
        }
      ],
      "classes": [],
      "callgraph": {
        "test_smoke": []
      },
      "dependencies": [],
      "hash": "a6845264444e66537bcf4ff95e3560a9deb6764c4ad8fda31f59580707d69a97",
      "artifact_id": "cd5aad59-c68e-4406-98f5-b5890df05a9d"
    },
    {
      "module_id": "orchestration.__init__",
      "path": "orchestration/__init__.py",
      "category": "test",
      "module_doc": "Modular tests for Multi-Agent Orchestration\n\nEpic 5 Phase 3.2: Beautiful modular test structure\nFollowing same patterns as orchestration implementation",
      "functions": [],
      "classes": [],
      "callgraph": {},
      "dependencies": [
        "create_mock_classification",
        "create_mock_router",
        "fixtures"
      ],
      "hash": "e5cd8b226d3d4505ef637758defdd16b2aca91d7cd2ed38360feac8add3c5e8a",
      "artifact_id": "29d3296f-f29e-4fda-b7a6-7dd762899fb1"
    },
    {
      "module_id": "orchestration.fixtures",
      "path": "orchestration/fixtures.py",
      "category": "test",
      "module_doc": "Shared test fixtures for orchestration testing\n\nEpic 5 Phase 3.2: Modular test utilities\nSingle responsibility: Common test setup utilities",
      "functions": [
        {
          "name": "create_mock_router",
          "docstring": "Create standardized mock ModelRouter for testing",
          "line_range": [
            15,
            39
          ],
          "parameters": [
            "model_name"
          ],
          "decorators": []
        },
        {
          "name": "create_mock_classification",
          "docstring": "Create standardized mock classification for testing",
          "line_range": [
            50,
            56
          ],
          "parameters": [
            "task_type",
            "confidence"
          ],
          "decorators": []
        },
        {
          "name": "mock_agent_routers",
          "docstring": "Standard set of mock agent routers for testing",
          "line_range": [
            60,
            66
          ],
          "parameters": [],
          "decorators": [
            "pytest.fixture"
          ]
        },
        {
          "name": "sample_task",
          "docstring": "Standard orchestration task for testing",
          "line_range": [
            70,
            79
          ],
          "parameters": [],
          "decorators": [
            "pytest.fixture"
          ]
        }
      ],
      "classes": [
        {
          "name": "MockClassification",
          "docstring": "Mock classification result for testing",
          "line_range": [
            43,
            47
          ],
          "methods": [],
          "bases": []
        }
      ],
      "callgraph": {
        "create_mock_router": [
          "Mock",
          "AsyncMock",
          "RoutingDecision"
        ],
        "create_mock_classification": [
          "MockClassification"
        ],
        "mock_agent_routers": [
          "create_mock_router"
        ],
        "sample_task": [
          "OrchestrationTask"
        ]
      },
      "dependencies": [
        "AsyncMock",
        "Mock",
        "ModelChoice",
        "ModelRouter",
        "OrchestrationTask",
        "RoutingDecision",
        "dataclass",
        "dataclasses",
        "pytest",
        "src.llmgenie.orchestration.core",
        "src.llmgenie.task_router",
        "unittest.mock"
      ],
      "hash": "336e9732d4a736487f06c17e382e2a4c901a3c91d1832fa7cdfa1275d305a844",
      "artifact_id": "45ee0175-ad13-45c7-9a08-cb679d046b75"
    },
    {
      "module_id": "orchestration.integration.__init__",
      "path": "orchestration/integration/__init__.py",
      "category": "test",
      "module_doc": "Integration tests\n\nEpic 5 Phase 3.2: Modular integration tests\n- Epic 5 component integration\n- Quality pipeline integration\n- Performance validation",
      "functions": [],
      "classes": [],
      "callgraph": {},
      "dependencies": [],
      "hash": "f6395a280dab5eed75eab816a2952b83e20a8cbc99d7bb71f286bee531d1b946",
      "artifact_id": "b0c10d66-06ad-409e-89b1-1483224f5174"
    },
    {
      "module_id": "orchestration.integration.test_epic5_components",
      "path": "orchestration/integration/test_epic5_components.py",
      "category": "test",
      "module_doc": "Test integration with Epic 5 components\n\nEpic 5 Phase 3.2: Modular integration tests\nSingle responsibility: Test Epic 5 TaskRouter/ModelRouter integration only",
      "functions": [
        {
          "name": "orchestrator_with_classifier",
          "docstring": "Create orchestrator with TaskClassifier integration",
          "line_range": [
            20,
            32
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.fixture"
          ]
        },
        {
          "name": "test_auto_mode_selection",
          "docstring": "Test automatic mode selection using TaskClassifier",
          "line_range": [
            35,
            47
          ],
          "parameters": [
            "self",
            "orchestrator_with_classifier"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_model_router_interface_compliance",
          "docstring": "Test that orchestration correctly uses ModelRouter interface",
          "line_range": [
            50,
            70
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_orchestrator_stats_with_classifier",
          "docstring": "Test orchestrator stats when TaskClassifier is available",
          "line_range": [
            72,
            79
          ],
          "parameters": [
            "self",
            "orchestrator_with_classifier"
          ],
          "decorators": []
        },
        {
          "name": "test_orchestrator_stats_without_classifier",
          "docstring": "Test orchestrator stats when TaskClassifier is not available",
          "line_range": [
            81,
            90
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "TestEpic5Integration",
          "docstring": "Test integration with Epic 5 TaskRouter components",
          "line_range": [
            16,
            90
          ],
          "methods": [
            {
              "name": "orchestrator_with_classifier",
              "docstring": "Create orchestrator with TaskClassifier integration",
              "line_range": [
                20,
                32
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_auto_mode_selection",
              "docstring": "Test automatic mode selection using TaskClassifier",
              "line_range": [
                35,
                47
              ],
              "parameters": [
                "self",
                "orchestrator_with_classifier"
              ]
            },
            {
              "name": "test_model_router_interface_compliance",
              "docstring": "Test that orchestration correctly uses ModelRouter interface",
              "line_range": [
                50,
                70
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_orchestrator_stats_with_classifier",
              "docstring": "Test orchestrator stats when TaskClassifier is available",
              "line_range": [
                72,
                79
              ],
              "parameters": [
                "self",
                "orchestrator_with_classifier"
              ]
            },
            {
              "name": "test_orchestrator_stats_without_classifier",
              "docstring": "Test orchestrator stats when TaskClassifier is not available",
              "line_range": [
                81,
                90
              ],
              "parameters": [
                "self"
              ]
            }
          ],
          "bases": []
        }
      ],
      "callgraph": {
        "orchestrator_with_classifier": [
          "AgentOrchestrator",
          "AsyncMock",
          "create_mock_router",
          "create_mock_classification",
          "Mock"
        ],
        "test_auto_mode_selection": [
          "create_mock_classification",
          "orchestrator_with_classifier.orchestrate"
        ],
        "test_model_router_interface_compliance": [
          "orchestrator.orchestrate",
          "AgentOrchestrator",
          "create_mock_router"
        ],
        "test_orchestrator_stats_with_classifier": [
          "len",
          "orchestrator_with_classifier.get_orchestration_stats"
        ],
        "test_orchestrator_stats_without_classifier": [
          "AgentOrchestrator",
          "orchestrator.get_orchestration_stats",
          "create_mock_router"
        ]
      },
      "dependencies": [
        "AgentOrchestrator",
        "AsyncMock",
        "ExecutionMode",
        "Mock",
        "TaskClassifier",
        "create_mock_classification",
        "create_mock_router",
        "fixtures",
        "pytest",
        "src.llmgenie.orchestration",
        "src.llmgenie.task_router",
        "unittest.mock"
      ],
      "hash": "6edc2562c8a82e152096729256d57da4f3c2ec9bd2172eab691f18e5b87ba0fe",
      "artifact_id": "77deb1cc-4694-40a6-84f9-f51984399b97"
    },
    {
      "module_id": "orchestration.core.test_task_models",
      "path": "orchestration/core/test_task_models.py",
      "category": "test",
      "module_doc": "Test OrchestrationTask and OrchestrationResult models\n\nEpic 5 Phase 3.2: Modular core tests\nSingle responsibility: Test task dataclass models only",
      "functions": [
        {
          "name": "test_orchestration_task_creation",
          "docstring": "Test basic task creation",
          "line_range": [
            21,
            37
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_orchestration_task_with_context",
          "docstring": "Test task creation with context",
          "line_range": [
            39,
            49
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_orchestration_task_with_subtasks",
          "docstring": "Test task creation with predefined subtasks",
          "line_range": [
            51,
            61
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_orchestration_result_creation",
          "docstring": "Test basic result creation",
          "line_range": [
            67,
            82
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_orchestration_result_with_quality_score",
          "docstring": "Test result creation with quality metrics",
          "line_range": [
            84,
            97
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_orchestration_result_metadata",
          "docstring": "Test result with comprehensive metadata",
          "line_range": [
            99,
            119
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_orchestration_result_status_values",
          "docstring": "Test valid status values",
          "line_range": [
            121,
            134
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_orchestration_result_efficiency_bounds",
          "docstring": "Test coordination efficiency is properly bounded",
          "line_range": [
            136,
            150
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "TestOrchestrationTask",
          "docstring": "Test OrchestrationTask dataclass",
          "line_range": [
            18,
            61
          ],
          "methods": [
            {
              "name": "test_orchestration_task_creation",
              "docstring": "Test basic task creation",
              "line_range": [
                21,
                37
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_orchestration_task_with_context",
              "docstring": "Test task creation with context",
              "line_range": [
                39,
                49
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_orchestration_task_with_subtasks",
              "docstring": "Test task creation with predefined subtasks",
              "line_range": [
                51,
                61
              ],
              "parameters": [
                "self"
              ]
            }
          ],
          "bases": []
        },
        {
          "name": "TestOrchestrationResult",
          "docstring": "Test OrchestrationResult dataclass",
          "line_range": [
            64,
            150
          ],
          "methods": [
            {
              "name": "test_orchestration_result_creation",
              "docstring": "Test basic result creation",
              "line_range": [
                67,
                82
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_orchestration_result_with_quality_score",
              "docstring": "Test result creation with quality metrics",
              "line_range": [
                84,
                97
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_orchestration_result_metadata",
              "docstring": "Test result with comprehensive metadata",
              "line_range": [
                99,
                119
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_orchestration_result_status_values",
              "docstring": "Test valid status values",
              "line_range": [
                121,
                134
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_orchestration_result_efficiency_bounds",
              "docstring": "Test coordination efficiency is properly bounded",
              "line_range": [
                136,
                150
              ],
              "parameters": [
                "self"
              ]
            }
          ],
          "bases": []
        }
      ],
      "callgraph": {
        "test_orchestration_task_creation": [
          "isinstance",
          "OrchestrationTask"
        ],
        "test_orchestration_task_with_context": [
          "OrchestrationTask"
        ],
        "test_orchestration_task_with_subtasks": [
          "len",
          "OrchestrationTask"
        ],
        "test_orchestration_result_creation": [
          "OrchestrationResult",
          "len"
        ],
        "test_orchestration_result_with_quality_score": [
          "OrchestrationResult"
        ],
        "test_orchestration_result_metadata": [
          "OrchestrationResult"
        ],
        "test_orchestration_result_status_values": [
          "OrchestrationResult"
        ],
        "test_orchestration_result_efficiency_bounds": [
          "OrchestrationResult"
        ]
      },
      "dependencies": [
        "AgentCoordination",
        "ExecutionMode",
        "OrchestrationResult",
        "OrchestrationTask",
        "datetime",
        "pytest",
        "src.llmgenie.orchestration.core"
      ],
      "hash": "a2fcc262d9288f06079ee513b812befe385aaef043e85f7e014c1b1dd722c9cb",
      "artifact_id": "6f965c64-50b4-4a91-b30a-bea3f3023a4d"
    },
    {
      "module_id": "orchestration.core.__init__",
      "path": "orchestration/core/__init__.py",
      "category": "test",
      "module_doc": "Core orchestration component tests\n\nEpic 5 Phase 3.2: Modular core tests\n- ExecutionMode functionality\n- AgentCoordination types  \n- Task/Result dataclasses",
      "functions": [],
      "classes": [],
      "callgraph": {},
      "dependencies": [],
      "hash": "8d4d25334b396f653cd7e2e27e12e65f8c6a9c11e31de4f9388e7fb8a33630f5",
      "artifact_id": "57c6266f-d965-4246-a13b-5dd8db2f8750"
    },
    {
      "module_id": "orchestration.core.test_execution_modes",
      "path": "orchestration/core/test_execution_modes.py",
      "category": "test",
      "module_doc": "Test ExecutionMode enum and smart suggestions\n\nEpic 5 Phase 3.2: Modular core tests\nSingle responsibility: Test ExecutionMode functionality only",
      "functions": [
        {
          "name": "test_execution_mode_values",
          "docstring": "Test that all execution modes have correct values",
          "line_range": [
            15,
            19
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_suggest_mode_for_collaborative_tasks",
          "docstring": "Test mode suggestion for collaborative tasks",
          "line_range": [
            21,
            31
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_suggest_mode_for_sequential_tasks",
          "docstring": "Test mode suggestion for sequential tasks",
          "line_range": [
            33,
            43
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_suggest_mode_for_parallel_tasks",
          "docstring": "Test mode suggestion for parallel tasks",
          "line_range": [
            45,
            55
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_mode_descriptions",
          "docstring": "Test mode descriptions are available",
          "line_range": [
            57,
            62
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_mode_use_cases",
          "docstring": "Test use cases are available for each mode",
          "line_range": [
            64,
            69
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        },
        {
          "name": "test_mode_enum_completeness",
          "docstring": "Test that we have expected number of execution modes",
          "line_range": [
            71,
            77
          ],
          "parameters": [
            "self"
          ],
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "TestExecutionMode",
          "docstring": "Test ExecutionMode enum functionality",
          "line_range": [
            12,
            77
          ],
          "methods": [
            {
              "name": "test_execution_mode_values",
              "docstring": "Test that all execution modes have correct values",
              "line_range": [
                15,
                19
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_suggest_mode_for_collaborative_tasks",
              "docstring": "Test mode suggestion for collaborative tasks",
              "line_range": [
                21,
                31
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_suggest_mode_for_sequential_tasks",
              "docstring": "Test mode suggestion for sequential tasks",
              "line_range": [
                33,
                43
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_suggest_mode_for_parallel_tasks",
              "docstring": "Test mode suggestion for parallel tasks",
              "line_range": [
                45,
                55
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_mode_descriptions",
              "docstring": "Test mode descriptions are available",
              "line_range": [
                57,
                62
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_mode_use_cases",
              "docstring": "Test use cases are available for each mode",
              "line_range": [
                64,
                69
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_mode_enum_completeness",
              "docstring": "Test that we have expected number of execution modes",
              "line_range": [
                71,
                77
              ],
              "parameters": [
                "self"
              ]
            }
          ],
          "bases": []
        }
      ],
      "callgraph": {
        "test_execution_mode_values": [],
        "test_suggest_mode_for_collaborative_tasks": [
          "ExecutionMode.suggest_mode"
        ],
        "test_suggest_mode_for_sequential_tasks": [
          "ExecutionMode.suggest_mode"
        ],
        "test_suggest_mode_for_parallel_tasks": [
          "ExecutionMode.suggest_mode"
        ],
        "test_mode_descriptions": [
          "isinstance",
          "len",
          "mode.get_description"
        ],
        "test_mode_use_cases": [
          "isinstance",
          "len",
          "mode.get_use_cases"
        ],
        "test_mode_enum_completeness": [
          "len",
          "list"
        ]
      },
      "dependencies": [
        "ExecutionMode",
        "pytest",
        "src.llmgenie.orchestration.core"
      ],
      "hash": "c0e34fcebe8e3ba2587f21831cb05149040ff783b53e99d7d2ed01d67b328794",
      "artifact_id": "92e14f2d-ff8d-4284-ae7b-d3c7516e02cd"
    },
    {
      "module_id": "orchestration.executors.test_parallel_executor",
      "path": "orchestration/executors/test_parallel_executor.py",
      "category": "test",
      "module_doc": "Test ParallelExecutor implementation\n\nEpic 5 Phase 3.2: Modular executor tests\nSingle responsibility: Test ParallelExecutor only",
      "functions": [
        {
          "name": "parallel_executor",
          "docstring": "Create ParallelExecutor with mock routers",
          "line_range": [
            20,
            26
          ],
          "parameters": [
            "self"
          ],
          "decorators": [
            "pytest.fixture"
          ]
        },
        {
          "name": "test_parallel_execution_success",
          "docstring": "Test successful parallel execution",
          "line_range": [
            29,
            37
          ],
          "parameters": [
            "self",
            "parallel_executor",
            "sample_task"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_parallel_execution_partial_failure",
          "docstring": "Test parallel execution with one agent failing",
          "line_range": [
            40,
            55
          ],
          "parameters": [
            "self",
            "parallel_executor",
            "sample_task"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_subtask_decomposition",
          "docstring": "Test automatic subtask decomposition",
          "line_range": [
            58,
            65
          ],
          "parameters": [
            "self",
            "parallel_executor"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "test_timing_efficiency",
          "docstring": "Test that parallel execution is actually parallel",
          "line_range": [
            68,
            85
          ],
          "parameters": [
            "self",
            "parallel_executor",
            "sample_task"
          ],
          "decorators": [
            "pytest.mark.asyncio"
          ]
        },
        {
          "name": "delayed_execute",
          "docstring": "",
          "line_range": [
            71,
            73
          ],
          "parameters": [],
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "TestParallelExecutor",
          "docstring": "Test parallel execution strategy",
          "line_range": [
            16,
            85
          ],
          "methods": [
            {
              "name": "parallel_executor",
              "docstring": "Create ParallelExecutor with mock routers",
              "line_range": [
                20,
                26
              ],
              "parameters": [
                "self"
              ]
            },
            {
              "name": "test_parallel_execution_success",
              "docstring": "Test successful parallel execution",
              "line_range": [
                29,
                37
              ],
              "parameters": [
                "self",
                "parallel_executor",
                "sample_task"
              ]
            },
            {
              "name": "test_parallel_execution_partial_failure",
              "docstring": "Test parallel execution with one agent failing",
              "line_range": [
                40,
                55
              ],
              "parameters": [
                "self",
                "parallel_executor",
                "sample_task"
              ]
            },
            {
              "name": "test_subtask_decomposition",
              "docstring": "Test automatic subtask decomposition",
              "line_range": [
                58,
                65
              ],
              "parameters": [
                "self",
                "parallel_executor"
              ]
            },
            {
              "name": "test_timing_efficiency",
              "docstring": "Test that parallel execution is actually parallel",
              "line_range": [
                68,
                85
              ],
              "parameters": [
                "self",
                "parallel_executor",
                "sample_task"
              ]
            }
          ],
          "bases": []
        }
      ],
      "callgraph": {
        "parallel_executor": [
          "ParallelExecutor",
          "create_mock_router"
        ],
        "test_parallel_execution_success": [
          "parallel_executor.execute",
          "len"
        ],
        "test_parallel_execution_partial_failure": [
          "parallel_executor.execute",
          "len",
          "Exception"
        ],
        "test_subtask_decomposition": [
          "subtask.lower",
          "parallel_executor._decompose_task",
          "any",
          "len"
        ],
        "test_timing_efficiency": [],
        "delayed_execute": [
          "asyncio.sleep"
        ]
      },
      "dependencies": [
        "Mock",
        "ParallelExecutor",
        "asyncio",
        "create_mock_router",
        "fixtures",
        "pytest",
        "sample_task",
        "src.llmgenie.orchestration.executors",
        "time",
        "unittest.mock"
      ],
      "hash": "ddcd4b096d4dc729b3e684bba692064d2767fe18826b66775122089d1140c562",
      "artifact_id": "ba367d41-da53-49ee-8d39-941b93c5ef12"
    },
    {
      "module_id": "orchestration.executors.__init__",
      "path": "orchestration/executors/__init__.py",
      "category": "test",
      "module_doc": "Executor strategy tests\n\nEpic 5 Phase 3.2: Modular executor tests\n- ParallelExecutor tests\n- SequentialExecutor tests  \n- CollaborativeExecutor tests",
      "functions": [],
      "classes": [],
      "callgraph": {},
      "dependencies": [],
      "hash": "d878102fd65f0224ee4e226de4fb6ffeaf2f4628b87f0d92753e3585ac2aa9dc",
      "artifact_id": "a0333ef7-1bb8-4411-a1e4-97e752d39d8b"
    }
  ]
}